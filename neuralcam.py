# -*- coding: utf-8 -*-
"""NeuralCam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ViSox1XjpoxPGmYyQnhFmE6XYP6aFHnH
"""

"""
NeuralCam
Copyright 2016, Byunggu Yu, All rights reserved.

Video cam program using deep LSTM instead of Kalman
Deep LSTM Long Short Term Memory Neural Network  
Written by Byunggu Yu, January 2017
"""
import numpy as np
import re	# Regix
import sys	
from scipy.special import expit as sigmoid
import matplotlib.pyplot as plt
import cv2

FrameWidth=0
FrameHeight=0
video_scale=0.25

#============= Global Constants and Variables ================
L=2			# number of hidden layers
hlayer_size = 200 	# number of neurons in a hidden layer
num_steps = 10		# BPTT step size
learning_rate = 0.1
init_scale = 0.1	# initial W is random uniform [-init_scale, init_scale]

input_dimension=None

wL=None
bL=None

W=[None]*L
H=[None]*L
B=[None]*L

#--- Adagrad mem params
mwL=None
mbL=None
mW=[None]*L
mB=[None]*L
#---

clean_c=[None]*L
clean_h=[None]*L

step=0
h, f, i, a, o, dh = {}, {}, {}, {}, {}, {}
c = {}
y, targets = {}, {}
thisC=None

#=============================================================

def setup(): #--- Initialize parameters at program start-up
  global input_dimension
  global W, wL
  global B, bL
  global mW, mwL
  global mB, mbL
  global clean_h, clean_c
  global h, c
  global thisC
  
  input_dimension = FrameWidth*FrameHeight # flattened array of the 2D frame
  print("Input Dimension=", input_dimension)
  
  #======= w, u, and b of each hidden layer
  W[0] = np.random.uniform(low=-init_scale, high=init_scale, size=(hlayer_size*4, input_dimension+hlayer_size))
  B[0] = np.zeros((hlayer_size*4, 1))
  for k in range(1,L): # for each hidden layer k,
    W[k] = np.random.uniform(low=-init_scale, high=init_scale, size=(hlayer_size*4, hlayer_size*2))
    B[k] = np.zeros((hlayer_size*4, 1))
  for k in range(0,L): # for each hidden layer k,
    clean_c[k] = np.zeros((hlayer_size, 1))
    clean_h[k] = np.zeros((hlayer_size, 1))
  
  #======= w and b of the readout layer
  wL = np.random.uniform(low=-init_scale, high=init_scale, size=(input_dimension, hlayer_size))
  bL = np.zeros((input_dimension, 1))
  c[-1]=np.copy(clean_c)
  h[-1]=np.copy(clean_h)
  thisC=0.0
  
  #======= initialize mw, mu, and mb for Adagrad ----
  mwL = np.zeros_like(wL)
  mbL = np.zeros_like(bL)
  for k in range(0,L): # for each hidden layer k,
    mW[k] = np.zeros_like(W[k])
    mB[k] = np.zeros_like(B[k])

def AdjustWeights(dwL, dbL, dW, dB): #--- Called by BackPropagation()
  global W, wL
  global B, bL
  global mW, mwL
  global mB, mbL
  # ================== Adjust w, u, and b ====================
  for param, dparam, mem in zip([wL, bL], [dwL, dbL], [mwL, mbL]):
    mem += dparam * dparam
    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update
  
  for param, dparam, mem in zip([W, B], [dW, dB], [mW, mB]):
    for k in range(0,L):
      mem[k] += dparam[k] * dparam[k]
      param[k] += -learning_rate * dparam[k] / np.sqrt(mem[k] + 1e-8) # adagrad update
	#===========================================================

def BackPropagation(): #--- Called by LSTM_filter()
  global dh
  
  #--- Memory Preparation
  for t in range(0, num_steps):
    dh[t]=[None]*L
  #-----------------------
  #---- initialize db, dw, du ----
  dwL = np.zeros_like(wL)
  dbL = np.zeros_like(bL)
  dW=[None]*L
  dB=[None]*L
  for k in range(0,L): # for each hidden layer k,
    dW[k] = np.zeros_like(W[k])
    dB[k] = np.zeros_like(B[k])
  #--------------------------------
  
  for t in reversed(xrange(num_steps)):
    dz=y[t]-targets[t]
    #dz=dz*(y[t]*(1-y[t]))
    dz=dz*y[t]*(1-y[t])
    #dz.__isub__(targets[t])
    
    dbL += dz
    
    dwL += np.dot(dz, h[t][L-1].T)
    
    if t==num_steps-1:
      dh[t][L-1]=np.dot(wL.T, dz)
    
    for l in reversed(range(0,L)):
      do=dh[t][l]*np.tanh(c[t][l])
      dc=dh[t][l]*(1-np.tanh(c[t][l])*np.tanh(c[t][l]))
      da=dc*i[t][l]
      di=dc*a[t][l]
      df=dc*c[t-1][l]
      
      dzf=df*f[t][l]*(1-f[t][l])
      dzi=di*i[t][l]*(1-i[t][l])
      dza=da*(1-a[t][l]*a[t][l])
      dzo=do*o[t][l]*(1-o[t][l])
      
      dz=np.concatenate((dzf, dzi, dza, dzo), axis=0)
      H = np.concatenate((h[t][l-1],h[t-1][l]),axis=0)
      dB[l]+=dz
      dW[l]+=np.dot(dz,H.T)
      dH=np.dot(W[l].T,dz)
      if l>0:
        dh[t][l-1]=dH[0:dH.shape[0]/2]
      if t>0:
        dh[t-1][l]=dH[dH.shape[0]/2:dH.shape[0]]

  #--- Optional Clipping
  for dparam in [dwL, dbL]:
    np.clip(dparam, -5, 5, out=dparam)
    for dparam in dW:
      np.clip(dparam, -5, 5, out=dparam)
    for dparam in dB:
      np.clip(dparam, -5, 5, out=dparam)
  #---------------------
  
  AdjustWeights(dwL, dbL, dW, dB)


def LSTM_filter(x):
  global step
  global h, f, i, a, o, c, y, targets
  global thisC
  
  print("step=%d, num_steps=%d\n" %(step, num_steps))
  
  #=========== Cost (loss) of the previous prediction
  if step >= 1:
    targets[step-1] = np.copy(x)
    thisC += 0.5*np.sum((y[step-1]-targets[step-1])*(y[step-1]-targets[step-1])) # loss in quadratic error


  #=========== Truncated Backpropagation Through Time (Truncated BPTT-SRN)
  if step == num_steps: # (num_steps)th x is the target of (num_steps-1)th x
    BackPropagation()

  #=========== Feedforward Through Time (FFTT-SRN)
  if step==num_steps: #--- Just finished the BackPropagation()
    step=0
    h[-1]=h[num_steps-1]
    c[-1]=c[num_steps-1]
    thisC=0

  #--- Memory Preparation
  h[step]=[None]*(L+1) # [-1] to store x
  f[step]=[None]*L
  i[step]=[None]*L
  a[step]=[None]*L
  o[step]=[None]*L
  c[step]=[None]*L

  #--- Hidden Layers ---
  h[step][-1]=x
  for l in range(0,L):

    H = np.concatenate((h[step][l-1],h[step-1][l]),axis=0)
    z=np.dot(W[l], H) + B[l]

    unit=z.shape[0]/4
    f[step][l]=sigmoid(z[0:unit])
    i[step][l]=sigmoid(z[unit:2*unit])
    a[step][l]=np.tanh(z[2*unit:3*unit])
    o[step][l]=sigmoid(z[3*unit:4*unit])

    c[step][l]=f[step][l]*c[step-1][l]+i[step][l]*a[step][l] 
    h[step][l]=o[step][l]*np.tanh(c[step][l]) 

  #--- Readout Layer ---
  zL=np.dot(wL, h[step][L-1]) + bL
  y[step] = zL # logit
  y[step] = sigmoid(zL) # no omega
    

  step = step+1
  return y[step-1], thisC

#==========================================================================#
if __name__ == "__main__":
#==========================================================================#

  cap1 = cv2.VideoCapture(0)
  ret1, frame1 = cap1.read()
  FrameWidth = int(frame1.shape[1]*video_scale)
  FrameHeight = int(frame1.shape[0]*video_scale)
  frame1 = cv2.resize(frame1, (FrameWidth, FrameHeight), interpolation = cv2.INTER_AREA)
  print ("Video Resolutions: ", FrameWidth, ", ", FrameHeight)
  cv2.namedWindow("Real Cam")
  cv2.moveWindow("Real Cam", FrameWidth, FrameHeight)
  cv2.namedWindow("Neural Cam")
  cv2.moveWindow("Neural Cam", FrameWidth*2, FrameHeight)
  
  #=== Pyplot variables
  plt.figure("deepLSTMfilter.py -- Byunggu Yu")
  #plt.ion()
  plot_x=[]
  plot_y1=[]
  plot_y2=[]
  #====================
  
  '''
  if len(sys.argv) != 2:
    print ("Usage: simple-rnn.py <input file>\n")
    exit(0)
  setup(str(sys.argv[1]))
  '''
  
  setup()
  
  num_of_steps = 0
  
  ea_C=1
  while True:
    # Capture frame-by-frame
    ret1, frame1 = cap1.read()
    frame1 = cv2.resize(frame1, (FrameWidth, FrameHeight), interpolation = cv2.INTER_AREA)

    # Our operations on the frame come here
    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY) #Each pixel becomes [0,255] int (np.uint8 type)
    # a frame is a 2D array (No. of Rows, No. of Columns)
    print(gray1.shape)

    #======= Convert the frame to a flattened array
    X = np.copy([gray1.flatten().astype(np.float)/255]).T
    
    #======= One Step: Sequential feedforwarding of X one by one and one BPTT at the end of the step
    LSTMframe, thisC = LSTM_filter(X)

    #======= Convert the return to the frame shape
    LSTMframe = (LSTMframe*255).astype(np.uint8)
    LSTMframe = LSTMframe.reshape(FrameHeight, FrameWidth)
    
    # Convergence Report
    ea_C = ea_C * 0.5 + (thisC/step) * 0.5
    
    # Pyplot Plotting
    plot_x.append(num_of_steps)
    if step ==1:
      plot_y1.append(thisC) #loss not found for step 0
    else:
      plot_y1.append(thisC/(step-1)) #loss not found for step 0
    plot_y2.append(ea_C)
    plt.clf()
    #plt.axis([0,num_of_steps,0,255])
    plt.plot(plot_x, plot_y1, label="Raw")
    plt.plot(plot_x, plot_y2, label="Exp Avg")
    plt.title("Quadratic Error")
    plt.legend(loc='upper right')
    plt.pause(0.0001)
    plt.draw()

    # Global counter per feed forwarding
    num_of_steps += 1
    
    # Display the frame
    cv2.imshow("Real Cam", gray1)
    cv2.imshow('Neural Cam', LSTMframe)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
      # When everything done, release the capture
      cap1.release()
      cv2.destroyAllWindows()
      #break
      exit()
